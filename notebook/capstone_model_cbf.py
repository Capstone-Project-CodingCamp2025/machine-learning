# -*- coding: utf-8 -*-
"""Capstone-Model-CBF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gDSLY49FzT35fym3kfHObvp-oLYPI6K7

# **1. Import library**

Menginstall semua depencies yang dibutuhkan
"""

!pip install Sastrawi
!pip install tensorflowjs

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# NLP Libraries
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler

# Text Processing
import re
import string
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

# Model Saving
import joblib
import json
import tensorflow as tf
import os

plt.style.use('default')
sns.set_palette("husl")

"""# **2. Load Dataset**"""

def load_data():
    """Load dataset dari file CSV"""
    from google.colab import files
    uploaded = files.upload()
    filename = list(uploaded.keys())[0]
    df = pd.read_csv(filename)

    return df

df = load_data()
print("Dataset berhasil dimuat!")
print(f"Shape: {df.shape}")
print("\nKolom:", df.columns.tolist())

"""# **3. Data Wrangling**

## 3.1. Statistics Descriptive
"""

# Ringkasan data
df.info()

df.describe()

"""## 3.2. Handling Missing Values"""

df.isnull().sum()

"""## 3.3. Handling Duplicated Data"""

df.duplicated().sum()

"""# **4. EDA**

## 4.1. Feature Distribution.
"""

def perform_eda(df):
    """Melakukan analisis data eksploratif"""

    # Visualizations
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # Rating distribution
    axes[0,0].hist(df['rating'], bins=20, alpha=0.7, color='skyblue')
    axes[0,0].set_title('Distribusi Rating')
    axes[0,0].set_xlabel('Rating')
    axes[0,0].set_ylabel('Frequency')

    # Reviews count distribution
    axes[0,1].hist(df['jumlah_ulasan'], bins=20, alpha=0.7, color='lightgreen')
    axes[0,1].set_title('Distribusi Jumlah Ulasan')
    axes[0,1].set_xlabel('Jumlah Ulasan')
    axes[0,1].set_ylabel('Frequency')

    # Rating vs Reviews scatter
    axes[1,0].scatter(df['rating'], df['jumlah_ulasan'], alpha=0.6)
    axes[1,0].set_title('Rating vs Jumlah Ulasan')
    axes[1,0].set_xlabel('Rating')
    axes[1,0].set_ylabel('Jumlah Ulasan')

    # Top locations by rating
    top_ratings = df.nlargest(5, 'rating')
    axes[1,1].barh(top_ratings['nama_tempat'], top_ratings['rating'])
    axes[1,1].set_title('Top 5 Tempat dengan Rating Tertinggi')
    axes[1,1].set_xlabel('Rating')

    plt.tight_layout()
    plt.show()

    return df

# Perform EDA
df = perform_eda(df)

"""# **5. Preprocessing**"""

class IndonesianTextPreprocessor:
    def __init__(self):
        # Initialize Sastrawi components
        self.stemmer = StemmerFactory().create_stemmer()
        self.stopword_remover = StopWordRemoverFactory().create_stop_word_remover()

        # Additional Indonesian stopwords
        self.additional_stopwords = {
            'wisata', 'alam', 'tempat', 'lokasi', 'indonesia', 'sumatera', 'utara',
            'kabupaten', 'kecamatan', 'kec', 'jalan', 'jl', 'desa', 'kelurahan'
        }

    def clean_text(self, text):
        """Membersihkan teks dari karakter khusus"""
        if pd.isna(text):
            return ""

        # Convert to lowercase
        text = str(text).lower()

        # Remove URLs
        text = re.sub(r'http\S+|www.\S+', '', text)

        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)

        # Remove punctuation and numbers
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\d+', '', text)

        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def remove_stopwords(self, text):
        """Menghapus stopwords"""
        # Use Sastrawi stopword remover
        text = self.stopword_remover.remove(text)

        # Remove additional stopwords
        words = text.split()
        words = [word for word in words if word not in self.additional_stopwords]

        return ' '.join(words)

    def stem_text(self, text):
        """Melakukan stemming"""
        return self.stemmer.stem(text)

    def preprocess(self, text):
        """Pipeline preprocessing lengkap"""
        text = self.clean_text(text)
        text = self.remove_stopwords(text)
        text = self.stem_text(text)
        return text

# Initialize preprocessor
preprocessor = IndonesianTextPreprocessor()

def preprocess_data(df):
    """Memproses data untuk model"""
    print("="*50)
    print("DATA PREPROCESSING")
    print("="*50)

    # Create processed dataframe
    processed_df = df.copy()

    # Handle missing values
    processed_df['rating'] = processed_df['rating'].fillna(processed_df['rating'].mean())
    processed_df['jumlah_ulasan'] = processed_df['jumlah_ulasan'].fillna(0)
    processed_df['content'] = processed_df['content'].fillna('')
    processed_df['alamat'] = processed_df['alamat'].fillna('')

    # Combine content and address for better feature extraction
    processed_df['combined_text'] = processed_df['content'] + ' ' + processed_df['alamat']

    # Preprocess text
    print("Memproses teks...")
    processed_df['processed_text'] = processed_df['combined_text'].apply(
        lambda x: preprocessor.preprocess(x)
    )

    # Create quality score (combination of rating and review count)
    scaler = MinMaxScaler()
    processed_df['normalized_reviews'] = scaler.fit_transform(
        processed_df[['jumlah_ulasan']]
    ).flatten()
    processed_df['quality_score'] = (
        processed_df['rating'] * 0.7 +
        processed_df['normalized_reviews'] * 0.3
    )

    print("Preprocessing selesai!")
    print(f"Sample processed text: {processed_df['processed_text'].iloc[0][:100]}...")

    return processed_df

# Preprocess data
processed_df = preprocess_data(df)

"""# **6. Feature Extraction**"""

def extract_features(processed_df):
    """Ekstraksi fitur menggunakan TF-IDF"""
    print("="*50)
    print("FEATURE EXTRACTION")
    print("="*50)

    # TF-IDF Vectorization
    tfidf_vectorizer = TfidfVectorizer(
        max_features=1000,  # Limit features for efficiency
        ngram_range=(1, 2),  # Unigrams and bigrams
        min_df=1,  # Minimum document frequency
        max_df=0.8,  # Maximum document frequency
        strip_accents='unicode',
        lowercase=True
    )

    # Fit and transform
    tfidf_matrix = tfidf_vectorizer.fit_transform(processed_df['processed_text'])

    print(f"TF-IDF Matrix Shape: {tfidf_matrix.shape}")
    print(f"Feature Names (first 10): {tfidf_vectorizer.get_feature_names_out()[:10]}")

    return tfidf_matrix, tfidf_vectorizer

# Extract features
tfidf_matrix, tfidf_vectorizer = extract_features(processed_df)

"""# **7. COMPUTING COSINE SIMILARITY**"""

def compute_similarity(tfidf_matrix):
    """Menghitung cosine similarity"""
    print("="*50)
    print("COMPUTING COSINE SIMILARITY")
    print("="*50)

    # Compute cosine similarity
    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

    print(f"Similarity Matrix Shape: {cosine_sim.shape}")
    print(f"Sample similarities for first item: {cosine_sim[0][:5]}")

    return cosine_sim

# Compute similarity
cosine_sim = compute_similarity(tfidf_matrix)

"""# **8. Evaluation**"""

class TourismRecommender:
    def __init__(self, df, cosine_sim, tfidf_vectorizer):
        self.df = df
        self.cosine_sim = cosine_sim
        self.tfidf_vectorizer = tfidf_vectorizer
        self.indices = pd.Series(df.index, index=df['nama_tempat']).drop_duplicates()

    def get_recommendations(self, place_name, top_n=5):
        """Mendapatkan rekomendasi berdasarkan nama tempat"""
        try:
            # Get index of the place
            idx = self.indices[place_name]

            # Get similarity scores
            sim_scores = list(enumerate(self.cosine_sim[idx]))

            # Sort by similarity (excluding the place itself)
            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]

            # Get indices of recommended places
            place_indices = [i[0] for i in sim_scores]

            # Create recommendation dataframe
            recommendations = self.df.iloc[place_indices].copy()
            recommendations['similarity_score'] = [score[1] for score in sim_scores]

            return recommendations[['nama_tempat', 'rating', 'jumlah_ulasan',
                                 'alamat', 'similarity_score']]

        except KeyError:
            return f"Tempat '{place_name}' tidak ditemukan dalam dataset."

    def get_recommendations_by_text(self, query_text, top_n=5):
        """Mendapatkan rekomendasi berdasarkan query text"""
        # Preprocess query
        processed_query = preprocessor.preprocess(query_text)

        # Transform query to TF-IDF
        query_tfidf = self.tfidf_vectorizer.transform([processed_query])

        # Compute similarity with all places
        query_sim = cosine_similarity(query_tfidf, tfidf_matrix).flatten()

        # Get top similar places
        similar_indices = query_sim.argsort()[-top_n:][::-1]

        # Create recommendation dataframe
        recommendations = self.df.iloc[similar_indices].copy()
        recommendations['similarity_score'] = query_sim[similar_indices]

        return recommendations[['nama_tempat', 'rating', 'jumlah_ulasan',
                             'alamat', 'similarity_score']]

# Initialize recommender
recommender = TourismRecommender(processed_df, cosine_sim, tfidf_vectorizer)

"""# **9. Inference**

"""

def test_recommendations():
    """Test sistem rekomendasi"""
    print("="*50)
    print("TESTING RECOMMENDATION SYSTEM")
    print("="*50)

    # Test 1: Recommendation by place name
    print("Test 1: Rekomendasi berdasarkan nama tempat")
    print("-" * 40)
    place_name = processed_df['nama_tempat'].iloc[0]
    print(f"Mencari rekomendasi untuk: {place_name}")

    recommendations = recommender.get_recommendations(place_name, top_n=3)
    print(recommendations)
    print("\n")

    # Test 2: Recommendation by query text
    print("Test 2: Rekomendasi berdasarkan query text")
    print("-" * 40)
    query = "bukit indah pemandangan alam"
    print(f"Query: '{query}'")

    text_recommendations = recommender.get_recommendations_by_text(query, top_n=3)
    print(text_recommendations)
    print("\n")

# Test the system
test_recommendations()

"""# **10. Simpan Model**

## Saved Model
"""

import os, json, numpy as np

# pastikan folder ada
os.makedirs("model", exist_ok=True)

# siapkan data TF-IDF
tv = tfidf_vectorizer

# cast vocabulary_ ke int biasa
vocab_py = {term: int(idx) for term, idx in tv.vocabulary_.items()}

obj = {
    "vocab": vocab_py,
    "idf":   [float(x) for x in tv.idf_],  # pastikan jadi Python float
    "terms": tv.get_feature_names_out().tolist()
}

# dump ke JSON
with open('model/tfidf.json','w') as f:
    json.dump(obj, f, ensure_ascii=False, indent=2)

# simpan similarity matrix sebagai .npy
np.save('model/cosine_sim.npy', cosine_sim)

# jika butuh JSON juga, convert inner ke float
cosine_list = [[float(x) for x in row] for row in cosine_sim]
with open('model/cosine_sim.json','w') as f:
    json.dump(cosine_list, f, ensure_ascii=False, indent=2)

import shutil

# Zip folder model menjadi cbf_encoder.zip
shutil.make_archive('model_cbf', 'zip', 'model')

from google.colab import files
files.download('model_cbf.zip')